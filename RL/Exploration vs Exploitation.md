## Major Component of RL Agent
  * Policy: Agent behavior function. It is mapping the state what you are in into the action you take.  
            We evaluate it a good or bad policy in terms of its expected discounted sum of reward.
    * Deterministic policy: $a = \pi(s)$
      There is simply one action per state
    * Stochastic policy: $\pi(a|s) = P[A_t = a | S_t =s]$
      You can have distribution over actions you may take.  
      
  * Value function: Future reward from being in a state /or action when following a particular policy.  
    $v_\pi (s) = E[G_t | S_t = s] = E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + ...| S_t = s]$ 
    Here $\gamma$ represents discounted factor, it weighs immediate vs long-term reward.(0,1).  
    If gamma is zero here, it indicates that we just care about immediate reward
    
  * Model: Agent's representation of how the world changes in respond to the agent's behavior.  
    $P_{SS'} = P[S_{t+1} = s' | S_t = s, A_t = a]$ P predict the next state.  [Given the state t, and an action a, what the probability of the next state].  
    $R_{S} = E[R_{t+1} = s' | S_t = s, A_t = a]$ R predict the next or immediate value.
    * Model-based : May or maynot have policy or value function. 
    * Model-free: No model  

## Online decision-making
How should an RL agent balance its actions?  
 * Exploitation : Make the best decision given current information.  
 * Exploration : Gather more information.  
   • The best long-term strategy may involve short-term sacrifices.  
   • Gather enough information to make the best overall decisions.  
 
 ##### Principle
 * Naive Exploration 
   * Add noise to greedy policy (e.g., ε-greedy) 
 * Optimistic Initialization 
   * Assume the best until proven otherwise 
 * Optimism in the Face of Uncertainty 
   * Prefer actions with uncertain values 
 * Probability Matching 
   * Select actions according to probability they are best Information 
 * State Search 
   * Lookahead search incorporating value of information

##### Multi-Armed Bandits
* A is a known set of actions. The environment generates a reward of R
* P[r|a] is a fixed but unknown distribution over the reward.
* The goal is to maximize the cumulative reward $\displaystyle\sum_{t=1}^t r_t$

###### Value and Regret
* The action value is the expected reward for an action: $Q(a) = E[R| A = a ]$.  
* The optimal value is : $V^* = Q(a^* ) = max Q(a) $.  
* The regret loss is the opportunity loss for one step : $I_t = E[V^* - Q(a^* )] $.  
* The regret loss of optimal action is zero.
* We want to maximize cumulative reward = minimize total regret $E[\displaystyle\sum_{t=1}^t V^* - Q(a^* )]$. 
* So the a = argmax(Q)
* The way we approximate Q in practice is by averaging the rewards received after we have selected a bunch of them
 <img width="620" alt="image" src="https://user-images.githubusercontent.com/29950267/215324408-0c2914dc-77a8-492b-a432-d436371dcef2.png">
  * I here represent indicates indicator, which means I(true) = 1 and I(false) = 0.
  * In the molecular, we have the summation of the indicator, it means the overall times you have selected this specific function.
  * When we compute it online we can still update it by storing the previous average and just moving whenever the new average emerge.[Incremental Way]
<img width="749" alt="image" src="https://user-images.githubusercontent.com/29950267/215325093-e96fa232-5f9f-4f79-8820-2da1d0b6349e.png">




