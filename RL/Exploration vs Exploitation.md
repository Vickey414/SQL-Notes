* Major Component of RL Agent
  * Policy: Agent behavior function. It is mapping the state what you are in into the action you take.  
            We evaluate it a good or bad policy in terms of its expected discounted sum of reward.
    * Deterministic policy: $a = \pi(s)$
      There is simply one action per state
    * Stochastic policy: $\pi(a|s) = P[A_t = a | S_t =s]$
      You can have distribution over actions you may take.  
      
  * Value function: Future reward from being in a state /or action when following a particular policy.  
    $v_\pi (s) = E[G_t | S_t = s] = E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + ...| S_t = s]$ 
    Here \gamma represents discounted factor, it weighs immediate vs long-term reward.(0,1).  
    If gamma is zero here, it indicates that we just care about immediate reward
    
  * Model: The representation of the world and how that changes in respond to the agent's behavior.
    R_{SS'} = P[S_{t+1} = s' | S_t = s, A_t = a]
    * Model-based : May or maynot have policy or value function. 
    * Model-free: No model
#### Online decision-making
How should an RL agent balance its actions
Exploitation : Make the best decision given current information.  
Exploration : Gather more information 


* Markov Process
* 




