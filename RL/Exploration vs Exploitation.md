## Major Component of RL Agent
  * Policy: Agent behavior function. It is mapping the state what you are in into the action you take.  
            We evaluate it a good or bad policy in terms of its expected discounted sum of reward.
    * Deterministic policy: $a = \pi(s)$
      There is simply one action per state
    * Stochastic policy: $\pi(a|s) = P[A_t = a | S_t =s]$
      You can have distribution over actions you may take.  
      
  * Value function: Future reward from being in a state /or action when following a particular policy.  
    * $v_\pi (s) = E[G_t | S_t = s] = E[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + ...| S_t = s]$   
    * Here $\gamma$ represents discounted factor, it weighs immediate vs long-term reward.(0,1).  
    * If gamma is zero here, it indicates that we just care about immediate reward
    
  * Model: Agent's representation of how the world changes in respond to the agent's behavior.  
    $P_{SS'} = P[S_{t+1} = s' | S_t = s, A_t = a]$ P predict the next state.  [Given the state t, and an action a, what the probability of the next state].  
    $R_{S} = E[R_{t+1} = s' | S_t = s, A_t = a]$ R predict the next or immediate value.
    * Model-based : May or maynot have policy or value function. 
    * Model-free: No model  

## Online decision-making
How should an RL agent balance its actions?  
 * Exploitation : Make the best decision given current information.  
 * Exploration : Gather more information.  
   • The best long-term strategy may involve short-term sacrifices.  
   • Gather enough information to make the best overall decisions.  
 
### Principle
 * Naive Exploration 
   * Add noise to greedy policy (e.g., ε-greedy) 
 * Optimistic Initialization 
   * Assume the best until proven otherwise 
 * Optimism in the Face of Uncertainty 
   * Prefer actions with uncertain values 
 * Probability Matching 
   * Select actions according to probability they are best Information 
 * State Search 
   * Lookahead search incorporating value of information

### Multi-Armed Bandits
* A is a known set of actions. The environment generates a reward of R
* P[r|a] is a fixed but unknown distribution over the reward.
* The goal is to maximize the cumulative reward $\displaystyle\sum_{t=1}^t r_t$

### Value and Regret
* The action value is the expected reward for an action: $Q(a) = E[R| A = a ]$.  
* The optimal value is :  $V^* = Q(a^* ) = max Q(a) $.  
* The regret loss is the opportunity loss for one step : $I_t = E[V^* - Q(a^* )] $.  
* The regret loss of optimal action is zero.
* We want to maximize cumulative reward = minimize total regret $E[\displaystyle\sum_{t=1}^t V^* - Q(a^* )]$. 
* So the a = argmax(Q)
* The way we approximate Q in practice is by averaging the rewards received after we have selected a bunch of them
 <img width="620" alt="image" src="https://user-images.githubusercontent.com/29950267/215324408-0c2914dc-77a8-492b-a432-d436371dcef2.png">
  * I here represent indicates indicator, which means I(true) = 1 and I(false) = 0.
  * In the molecular, we have the summation of the indicator, it means the overall times you have selected this specific function.
  * When we compute it online we can still update it by storing the previous average and just moving whenever the new average emerge.[Incremental Way]
<img width="620" alt="image" src="https://user-images.githubusercontent.com/29950267/215325093-e96fa232-5f9f-4f79-8820-2da1d0b6349e.png">


### Greedy Algorithm
* That the greedy policy has linear regret means that an expectation the regret grows as a function that is linear to the number of steps you have take.
* Compare:
  * The greedy policy can lock into a suboptimal action forever.  
  * The $\varepsilon$-greedy policy continue to explore forever.  
* The $\varepsilon$-greedy algorithm
* 例子： 在一家餐厅里面，客人来了不用点单，用算法决定给客人选哪道菜,菜做得好吃的概率为p，不好吃为1-p,菜做的好吃时，客人留下奖励 = 1，菜不好吃时奖励为0
* 解决思路  
>> 探索阶段 (Exploration)：通过多次观测推断出一道菜做的好吃的概率－如果一道菜已经推荐了k遍（获取了k次反馈），我们就可以算出菜做的好吃的概率：
>> p = $\displaystyle\sum_{reward_i} \over k $
>>> 如果推荐的次数足够多，k足够大，那么p(heart)会趋近于真实的菜做的好吃的概率p
>> 利用阶段 (Exploitation)：已知所有的菜做的好吃的概率，该如何推荐？－ 如果每道菜都推荐了多遍，我们就可以计算出N道菜做的好吃的概率{$p_1$,$p_2$,$p_3$...,$p_N$}，那么我们就可以推荐p(heart)最大的那道菜。
>>> Exploration的代价是要不停的拿用户去试菜，影响客户的体验，但有助于更加准确的估计每道菜好吃的概率
>>> Exploitation会基于目前的估计拿出“最好的”菜来服务客户，但目前的估计可能是不准的（因为试吃的人还不够多）
   * With the probability of 1- $\varepsilon$ select the greedy action. [Exploration]
   * with the probability of $\varepsilon$ select the random action. [Exploitation]

