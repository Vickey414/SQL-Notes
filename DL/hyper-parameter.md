
1. 先overfit再tradeoff，先保证模型capacity能够过拟合，再尝试使用正则化的方法减小模型。  
2. learning rate是非常重要的参数，一般nlp bert类模型在$e^{-5}$级别附近，cv类模型在$e^{-3}$级别附近   
3. batch size，在学习领域一般越大越好，否则模型可能不收敛。  
4. dropout：现在一般模型都需要使用预训练模型  
5. 初始化方法：linear，cnn一般选用kaiming uniform或者normalize，embedding一般选用截断normalize  
6. 随机数种子设定好，否则很多对比实验结论不一定准确  
7. cross validation方式要结合任务设计，数据标签设计，其中时序数据要避免未来信息泄漏。 
8. 优化器：NLP等抽象层次比较高或者目标函数非常不平滑的问题选择adam优先，其他的尝试使用SGD。 
9. 不要过早的early-stopping。  


参考文章：https://blog.csdn.net/ytusdc/article/details/122321907  
Pytorch训练模型损失Loss为Nan或者无穷大（INF）原因
1. 比如求损失函数会用到log(x)，如果 x 接近0，那么结果就是 inf。
* 梯度消失：是指导数值特别小，导致其连乘项接近无穷小，可能是由输入数据的值域太小（导致权重 W 的导数特别小）或者是神经网络层输出数据落在在激活函数的饱和区（导致激活函数的导数特别小）.  
* 梯度爆炸：是指导数值特别大，导致其连乘项特别大，致使w在更新后超出了值域的表示范围。可能是输入数据没有进行归一化（数据量纲太大致使W的梯度值变大），只要连乘项的导数一直大于1，就会
使得靠近输入层的W更新幅度特别大。连乘项是指链式求导法则中毎一层的导数，很明显梯度消失与梯度爆炸都受连乘项的影响（也就是网络梯度的影响)。  

##### 总结：梯度消失不会导致模型出现nan和inf ，只会导致模型loss不会下降，精度无法在训练过程中提升。而梯度爆炸则有可能导致模型在训练过程中出现 inf。出现情形有上溢出和下溢出:  
* 上溢出：首先怀疑模型中的指数运算，因为模型中的数值过大，做exp(x)操作的时候出现了上溢出现象，这里的解决方法是推荐做Nrom操作，对参数进行正则化，这样在做exp操作的时候就会很好的避免
出现上溢出的现象，可以做LayerNormBatchNorm 等，这里我对模型加fine-tune的时候使用LayerNorm 解决了loss为NAN 的问题。【比如不做其他处理的softmax中分子分母需要计算exp（x），
值过大，最后可能为INF/INF，得到NaN，此时你要确认你使用的softmax中在计算exp（x）做了相关处理（比如减去最大值等等)】
* 上溢出: 同样可能是因为, x/0 的原因，这样就不是参数的值过大的原因，而是具体操作的原因，例如，在自己定义的softmax类似的操作中出现问题，下面是softmax解决上溢出和下溢出的解决方法：
* 下溢出：一般是log⁡(0) 或者exp(x)操作出现的问题。可能的情况可能是学习率设定过大，需要降低学习率，可以降低到学习率直至不出现nan为止，例如将学习率1e-4设定为1e-5即可。

* 除了降低学习率的方法，也可在在优化器上面加上一个eps来防止分母上出现0的现象，例如在batchnorm中就设定eps的数值为1e-5，在优化器同样推荐加入参数eps,torch.optim.adam中默认的eps
是1e-8。但是这个值属实有点小了，可以调大这个默认的eps 值，例如设定为1e-3。  

```python
optimizer1 = optim.Adam(model.parameters(), lr=1e-3, eps=1e-4)
optimizer1 = optim.Adam(model.parameters(), lr=0.001, eps=1e-3)
optimizer2 = optim.RMSprop(model.parameters(), lr=0.001, eps=1e-2)
```

#### 从数据的角度，造成nan或者inf的原因有：  
* 对于输入数据有缺陷，模型前向传播过程中值域超出界限，需要用均值进行 nan 值填充，对于图像数据通常不会出现。
* 绝大部分情况是值域的问题，值过大或过小。这出现在模型的前向传播中，模型中的系列运行使得数据超出值域范围，如交又熵中的log函数， $log(e^{-10})$ 值过小; 
mse loss中的 $x^2$取平方值过大；激活函数中的relu函数，没有上界限制,模型的权重过大。


