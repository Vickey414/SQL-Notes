# Bagging
 Based on two key points : boostrap and aggregation.  
 Bagging create training subsets using bootstrap sampling.  
 ## Boostrapping
 >> To create a new “base learner" $f_b$ ,
  >>> 1. we randomly draw with replacement a dataset $D_b$ of ntrain observations from the training set
  >>> 2.we learn the method (ex: CART) on it → the “base learner" fb is obtained
  >>> Note: each $D_b$  has the same size as the original training set

* 背景【Why we use bagging to tackle down problem of decision tree】：
  * The decision trees discussed in suffer from high variance. This means that if we split the training data into two parts at random, and ﬁt a decision tree to both halves, the results that we get could be quite diﬀerent.  
  * What we want is that a procedure with low variance that will yield similar results if applied repeatedly to distinct data sets.  
  * Linear regression tends to have low variance if the ratio of n to p is moderately large(就是regression只要n足够大的情况下variance就会小). Bootstrap 
aggregation,orbagging, is a general-purpose procedure for reducing the variance of a statistical learning method
 
## Steps of bagging
> Consists to
  >> * 1. Do boostrapping B times producing B bootstrap datasets $D_b$ 
  >>> then B predictors (“base learners") fb for each of these datasets
  >> * 2. Aggregate the predictors
<img width="640" alt="image" src="https://user-images.githubusercontent.com/29950267/215358842-7c1439a4-7d56-4bbc-a3e0-6c07d4e5dbec.png">


# Random Forest
  <img width="640" alt="image" src="https://user-images.githubusercontent.com/29950267/215359731-104265fb-4a7b-460c-942a-6f5cd36bc75a.png">

#### Definition.  
* A random forest is a set of trees growing on a boostrapped learning data set and where, before each split, a set of m d input variables (or features) is randomly selected as candidates for splitting. 

