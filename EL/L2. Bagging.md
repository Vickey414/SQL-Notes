# Bagging
 Based on two key points : boostrap and aggregation.  
 Bagging create training subsets using bootstrap sampling.  
 ## Boostrapping
 >> To create a new “base learner" $f_b$ ,
  >>> 1. we randomly draw with replacement a dataset $D_b$ of ntrain observations from the training set
  >>> 2.we learn the method (ex: CART) on it → the “base learner" fb is obtained
  >>> Note: each $D_b$  has the same size as the original training set

* 背景【Why we use bagging to tackle down problem of decision tree】：
  * The decision trees discussed in suffer from high variance. This means that if we split the training data into two parts at random, and ﬁt a decision tree to both halves, the results that we get could be quite diﬀerent.  
  * What we want is that a procedure with low variance that will yield similar results if applied repeatedly to distinct data sets.  
  * Linear regression tends to have low variance if the ratio of n to p is moderately large(就是regression只要n足够大的情况下variance就会小). Bootstrap 
aggregation,or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method
 
## Steps of bagging
> Consists to
  >> * 1. Do boostrapping B times producing B bootstrap datasets $D_b$ 
  >>> then B predictors (“base learners") fb for each of these datasets
  >> * 2. Aggregate the predictors
<img width="640" alt="image" src="https://user-images.githubusercontent.com/29950267/215358842-7c1439a4-7d56-4bbc-a3e0-6c07d4e5dbec.png">


# Random Forest
  <img width="640" alt="image" src="https://user-images.githubusercontent.com/29950267/215359731-104265fb-4a7b-460c-942a-6f5cd36bc75a.png">
  
#### Definition.  
* A random forest is a set of trees growing on a boostrapped learning data set and where, before each split, a set of m d input variables (or features) is randomly selected as candidates for splitting. 

#### Attributes
* Reducing m will reduce the correlation between any pairs of trees in the ensemble. Thus reducing the variance of the average
* for regression,choose m = $\frac{d}{3}$ and a minimum node size of 5
* for classification, choose m = $\sqrt{d}$ and a minimum node size of 1


#### Evaluation 
<img width="640" alt="image" src="https://user-images.githubusercontent.com/29950267/223057023-07985076-dc8b-4e7a-b36e-c818d1c8585d.png">
Out-of-Bag Error(OOB) advantage: 不需要用到cross validation
* An OOB-error estimate is almost the identical to that obtained by N-folder cross-validation
* Once the OOB-error stabilizes, the training can be terminated and B value is obtained/tunned

#### Anomalies Detection
* RFs are well suited to detecting outliers.
* The anomaly score of an observation $x_i$ is determined approximately by the average length of the path from $x_i$ to the leaves of trees in the forest
就是判断是不是异常值可以根据最终tree path的长度，The shorter the path, the more likely the observation is atypical

#### Cons of random forest
* black box: difficult to interpret
* slower training

boosting的算法过程如下：

1、对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。
2、进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）

Boosting系列算法最经典的包括AdaBoost算法和GBDT算法。 
Boosting是一种递进的组合方式，每一个新的分类器都在前一个分类器的预测结果上改进，所以说boosting是减少bias而bagging是减少variance的模型组合方式。

